\documentclass[letterpaper,twocolumn,10pt]{article}
\usepackage{usenix-2020-09}
    
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{algpseudocode}
\usepackage{algorithm}

\begin{document}
%don't want date printed
\date{}

\title{\Large \bf OneRuleToFindThem: Efficient Automated Generation of Password Cracking Rules}

\author{
{\rm Joshua Eckroth}\\
Stetson University
\and
{\rm Lannie Hough}\\
i2k Connect, Inc.
\and
{\rm Hala ElAarag}\\
Stetson University
}

\maketitle

\begin{abstract}
Password cracking tools such as Hashcat support the use of rules that transform
a dictionary of words, such as common English words and previously-cracked
passwords, into new candidate guesses. Rules are necessary to achieve high
cracking ratios, however, they are difficult and time-consuming to build by hand. We have
developed an algorithm and implementation that automatically finds successful
rules by combinatorial generation of rules and empirical observation of how
often each generated rule transforms a dictionary word to a target password.

Our algorithm makes numerous performance and logical optimizations to avoid the
numerous pitfalls that would occur if a naive brute-force technique was used.  In this
paper we explain our algorithm in detail and experimentally compare the performance of
its outputs to existing rule sets constructed via various approaches ranging from
fully-manual to fully-automated like our own.

We show that our approach is completely automated and achieves comparable
cracking performance to other top rule sets while also generating rules
that don't exist in other rule sets. This makes cracking attempts using our
rules mostly complementary to cracking attempts with other rule sets and
suggests that the use of our rules can augment the use of other rules.

\end{abstract}


\section{Introduction}

Although not the only form of authentication, the most common form of authentication for
applications is passwords. Properly implemented password authentication software utilized
by users who select very strong passwords, such as a sufficiently long password composed
of random characters, is generally effective. However, it is well-known that a significant
proportion of people elect to use word or phrase-based passwords with a few numbers or
special characters if required by the software in use. These passwords often have predictable
patterns and evolve in predictable ways over time as a user is forced to change their
password, often resulting in simple modifications resulting in a new password with a high
degree of similarity to the original.\cite{hanamsagar2018leveraging} While this approach
might make passwords more amenable to memorization, it significantly weakens them in the
face of a password cracking attack.

A common approach to cracking passwords today is through the use of a program called
hashcat, using the technique of hashing candidate passwords in a wordlist and checking
if the hash matches one found in a file containing password hashes obtained via the
data breach of some service. In order to avoid requiring a massive exhaustive wordlist,
hashcat uses `rules' which map to functions that perform some transformation on a password.
This transformation is then hashed and the same check is made. Examples include reverse (r),
append character (\$X), and replace (sXY).\cite{hashcat}

% https://hashcat.net/wiki/doku.php?id=rule_based_attack
% https://hashcat.net/hashcat/

To produce the most guesses and therefore crack the most passwords it is to the benefit of
an attacker to have a large number of rules. However, the fact that most passwords are
not generated randomly ensures that some rules will be (sometimes dramatically) more effective
than others. Because each additional rule used increases the time the cracking process takes
to complete an attacker is very likely to want to optimize for the \# cracked/time metric
and prioritize only effective rules in their rule set.

% TODO: better division, cracked/time

Many effective lists of rules already exist and some are in fact distributed with the
hashcat software, such as the sizable dive.rule list. Various approaches have been
taken to produce these lists. Some individuals manually curate rules, which is a 
time-consuming process, and others have tried various algorithmic and automated approaches.
Oftentimes existing rule sets are aggregated to various degrees to produce a `super rule',
famously OneRuleToRuleThemAll and also some of the highly-effective Pantagrule lists of rules.\cite{ortrta}\cite{pantagrule}
The purpose of this paper is to detail a novel fully-automated approach to rule generation that
we've invented, incorporating findings from previous research into password strength. We then
compare the efficacy of both our technique and of our generated rule sets to existing rules.

% existence of aggregated rule sets (oneruletorulethemall, pantagrule)

% password change policies (regular intervals) might encourage people to make
% simple modifications to their existing password to create a new one; we want
% to find these by trying a trivial (primitive) modification one at a time until
% we hit a known password

% this approach has benefits because it can find unexpected combinations of
% primitive rules that might actually be somewhat common, however this approach
% also has the drawback of

\section{Related Work}

% TODO: PACK, pantagrule and NSA-rules
% TODO: explain NSA more?
Existing automated approaches to generating rules do exist. Several are based on the PACK toolkit,
a collection of tools designed for analyzing password lists to detect masks, rules, character sets,
and various other password characteristics that can produce results designed to work with Hashcat.
The nsa-rules analysis and the rules it generates take advantage of this toolkit as well as the more
effective Pantagrule rules.\cite{PACK}\cite{NSAKEY}\cite{pantagrule}

Pantagrule rules were generated using PACK's Levenshtein reverse path algorithm to produce rules which
were then sorted by PACK rule generation frequency. This is similar to the approach NSAKEY took but
Pantagrule used a larger set of base passwords to generate rules, which although initially public is
now inaccessible. To further optimize the rules Pantagrule ran the top generated rules against the
Pwned Passwords NTLM list using the rockyou wordlist and ineffective rules were discarded. Various
rule lists are created from rules generated by various subsets of the seed data including top passwords,
random passwords, and a hybrid of the two.

A `one.rule' Pantagrule rule list builds upon the popular
OneRuleToRuleThemAll rule list which was created by concatenating and de-duping the top 25\% of rules
from various other rule lists. `one.rule' appends top performing Pantagrule hybrid rules to OneRuleToRuleThemAll
and truncates the list to the size of `dive.rule', a popular rule list distributed with hashcat.
OneRuleToRuleThemAll exceeds the performance of dive on its own, both in total \% of passwords cracked and in
cracking efficiency on the Lifeboat data dump. Pantagrule's one.rule also compares favorably in total number of
passwords cracked against dive at the same total number of rules and against OneRuleToRuleThemAll as a superset
of OneRule's rules, however it is less efficient than OneRule as a consequence of containing significantly more
total passwords. Pantagrule suggests that their `one' peforms better than other known lists the size of dive
and they recommend it as a first list to try.


% TODO: two ml approaches
In addition to traditional rule-based approaches to guessing passwords, some techniques have
been developed that attempt to avoid this entirely.  PassGAN is an approach that attempts to replace rule-based
password guessing with an approach based on deep learning and generative adversarial networks (GANs).
A neural network was trained to determine password characteristics and structures without making any
assumptions about these. Like our approach, PassGAN makes use of part of the rockyou dataset and
trains on it; they then tested their results against both rockyou (with training data removed)
and a dataset of LinkedIn passwords. Their results show that their approach is able to match 34.6\% of
passwords in the rockyou dataset and 34.2\% in the LinkedIn dataset. While a typical rule-based attack
has the disadvantage of being able to exhaust guesses once all rules have been applied to all initial
passwords, PassGAN can generate guesses effectively forever. So while PassGAN can in theory eventually
guess more passwords than any other approach, it needs to generate significantly more passwords to do
this. PassGAN also matches some passwords not matched by any password rule in the rule sets they
compared against.\cite{hitaj2019passgan}

Foo~\cite{pasquini2021improving} is

- model the representation of passwords in the latent space of a GAN and of
Wasserstein Auto-Encoders
    - so semantically similar passwords are closer
- using this representation, they can find passwords with strong locality and
with weak locality
- about PassGAN:
    - requires up to 10x more guesses to reach same number of matched passwords
as competitors
    - only considered passwords <= 10 characters
- their GAN has a big improvement over PassGAN
- locality is used to sample where to look next for password guessing

\section{Algorithm}
\label{sec:algorithm}

Our rule generator requires two inputs: a set of rule primitives that will be
combined to form complex rules, and a set of target passwords such as the
Rockyou list. We implement an efficient version of what is essentially a
brute-force procedure. We first describe the brute-force procedure and then
describe our optimizations.


\subsection{Brute-force procedure}

Given each initial target password (e.g., from Rockyou), we apply every
primitive rule to the password to generate new passwords. For example, the
primitive Hashcat rule `r' (reverse) applied to the initial target password
`123456' results in password `654321.' We use a primitive rule set consisting of
elementary operations such as reverse (`r'), remove last character (`]'), delete
all `s' characters (`@s'), and so on, totaling nearly 400 primitive rules. The
selected password is subjected to every primitive, resulting in about 400 new
passwords. For each resulting password (such as `654321'), we check if it is one
of our targets from our initial list of targets (e.g., Rockyou). If it is, we
boost the score of the rule that was applied. In the end, we have a list of
rules with scores indicating which rules were most successful.

After that initial step of applying rules to a single password, we proceed to
choose another password and apply all primitive rules to it, boosting the scores
of rules that transform the password to a known target password. We choose the
next password to try according to an ordering of the original target list (e.g.,
Rockyou sorted by password `strength,' with weaker passwords chosen earlier;
details are given below).

Each password that is generated from applying primitive rules becomes a
potential candidate itself, unless it is already known from the initial target
set. For example, if the rule `r' is applied to `foobar,' producing `raboof,'
and `raboof' is not already known from the target set, it becomes a candidate
for selection. We record the history of rules that have already been applied, in
this case just `r.' When `raboof' is eventually selected as the next password to
try, each primitive rule is appended to its rule history, producing complex
rules `r ]' `r @s' and so on. If `]' applied to `raboof,' which produces
`raboo,' is a target, then we boost the score of the complex rule `r ].' We note
that the initial password `foobar' (pulled from Rockyou) was transformed to
`raboo' using complex rule `r ]' and `raboo' is a target (in this example,
though in reality it is not a member of Rockyou). Thus, our procedure has
discovered a successful rule that should be utilized in password cracking.

\begin{algorithm}\caption{Brute-force procedure, without optimizations}
\begin{algorithmic}[1]
\State $PrimitiveRules \gets $ fileContents(``primitives.rule'')
\State $Rules \gets PrimitiveRules$
\State $Targets \gets $ fileContents(``rockyou.txt'')
\ForAll {$p \in Targets$}
  \State $\mathrm{setRuleHistory}(p, \{\})$
\EndFor
\State $Candidates \gets Targets$
\State $Processed \gets \{\}$
\While{$|Candidates|\geq 0$}
  \State $p \gets \mathrm{chooseOne}(Candidates)$
  \State $Candidates \gets Candidates \setminus \{p\}$
  \State $Processed \gets \{p\} \cup Processed$
  \ForAll {$r \in PrimitiveRules$}
    \State $p' \gets \mathrm{applyRule}(p, r)$
    \State $H \gets \{r\}\cup\{\mathrm{append}(h, r)|h \in
\mathrm{ruleHistory}(p)\}$
    \State $\mathrm{setRuleHistory}(p', H)$
    \If {$p' \in Targets$}
      \ForAll {$h \in H$}
        \If {$h \in Rules$}
          \State $s \gets \mathrm{getScore}(h)$
          \State $\mathrm{setScore}(h,
s+\mathrm{strength}(p'))$
        \Else
          \State $\mathrm{setScore}(h, \mathrm{strength}(p'))$
          \State $Rules \gets \{h\}\cup Rules$
        \EndIf
      \EndFor
    \EndIf
    \If {$p' \notin Processed \cup Candidates$}
      \State $Candidates \gets \{p'\}\cup Candidates$
    \EndIf
  \EndFor
\EndWhile
\end{algorithmic}
\label{alg:brute-force}
\end{algorithm}

In summary, the brute-force procedure begins with an initial list of target
passwords and puts them into a candidate set, picks a single candidate password
at a time and applies all primitive rules, and boosts the scores of any rules
that ultimately produced a password found in the initial list of targets. Each
password generated from applying rules goes into the candidate set if it is not
already in there, and the sequence of primitive rules that generated it is
associated with the password.

\begin{figure}
\includegraphics[width=\linewidth]{example-pw-rules.pdf}
\caption{Small example of the combinatorial explosion of passwords generated by
applying primitive rules. Note that some passwords may be reached by several
distinct rule histories, e.g., starting with password `123456,' the password
`54321' may be arrived at by applying complex rules `r [' or `] r,' or
even `\$! r [ [' (not shown in the graph).}
\label{fig:pwgen}
\end{figure}

Figure~\ref{fig:pwgen} shows an example of the combinatorial explosion of
candidates that results from the brute-force algorithm.

\subsection{Optimizations for effectiveness}

The brute-force procedure suffers from significant drawbacks. Since it lacks
any criteria for checking rule validity and structure or for preferring to
examine some passwords before others, it is likely to generate worthless rules
and take a long time to do so.

\subsubsection{Hit a target only once}

The rockyou wordlist, which is our input to the algorithm, includes some very
basic words like `password' and even the single letter `a.' The brute-force
procedure will discover rules such as `] ] ] ] ] ] \$a' that will transform any
six-character password such as `gh\%@\_\$' into the password `a,' and the
procedure will boost the score of that rule. But that rule is hardly effective
for cracking password hashes. However, the procedure will boost that rule for
every six-character candidate because the rule will hit a target (namely, the
target `a'). When we allow this behavior, we see that the procedure yields
abundant variations of this logic (erasing characters from either end, then
adding a few to hit a small target), and they are not effective in experiments.

An easy way to prevent this behavior is to modify the `if' block starting on
line~17 in Algorithm~\ref{alg:brute-force} to what is shown in
Algorithm~\ref{alg:hit-target-once}.

\begin{algorithm}\caption{Hit a target only once}
\begin{algorithmic}
    \If {$p' \in Targets$}
      \ForAll {$h \in H$}
        \State \dots
      \EndFor
      \State $Targets \gets Targets \setminus \{p'\}$
    \EndIf
\end{algorithmic}
\label{alg:hit-target-once}
\end{algorithm}

\subsubsection{Ordering by password strength}

Because our algorithm applies all primitive rules to each candidate password,
we will produce hits faster if the candidate passwords we choose are those that
are the most likely to be transformed into a target password. Intuitively it makes
sense that the application of primitive rules to already very strong passwords, such as long
passwords consisting of random characters, would be less effective than the application
of rules to weaker passwords. In order to select these weaker passwords earlier in
our rule generation procedure we first generate individual password strengths.

For individual password strength we utilize a metric invented by Joseph Bonneau called
the `partial guessing metric' which was compared to other metrics and determined
to be particularly effective. Important properties of this metric are that it provides
equal strength to all passwords in a uniform distribution UN where each of N events
are equally likely and that it rates any event more weakly than events less common in
the distribution X. 

This metric is developed with the assumption that the
population-wide distribution X of passwords is completely known and addresses the issue
of estimating the strength of previously unseen passwords when a sample is used as an approximation
of X. For our approximation of X we make use of the passwords in the rockyou dataset and the
frequency at which they appear as a sample. We produce a mapping of the passwords in our distribution
to their strengths and provide for estimating the strength of unseen passwords, allowing us
to provide each candidate encountered with a strength value.

With strength values known for all initial candidates and the ability to determine the strength
of new candidates we can create a priority queue where high priority candidates are those
with a low strength value. We select these weaker candidates first.

% also mention scoring rules w/ strength?

\cite{bonneau2012statistical}

reducing strength for generated but unknown passwords


\subsubsection{Rule simplification}

The brute-force procedure appends each primitive rule to each rule in a
password's rule history on line~15. For example, if the password `password123'
was reached by iterative appending of primitive rules `\$1,' then `\$2,' then
`\$3,' the password will have rule history `\$1 \$2 \$3' (among others,
possibly). If `password123' is later selected as a candidate, each primitive
rule will be added to the end of that history and tested to see if it hits a
target. For example, `\$4' will be added and since `password1234' is a target,
each rule in the history (with `\$4' appended) will be boosted. Thus, the rule
`\$1 \$2 \$3 \$4' will be boosted.

We have identified numerous conditions in which complex rules (sequences of
primitive rules) are equivalent to a simpler rule. For example, the rule `\$1 ]
\$a' is equivalent to `\$a.' We also normalize rules by reordering some
sequences of primitives. For example, the rule `\^{}2 ] \^{}1' is equivalent to
`\^{}2 \^{}1 ]' (they both insert `12' at the front and remove the last
character). If we normalize all rules according to some common simplification
and sequencing logic, we can be sure to boost the normalized version of a rule
instead of boosting different variations and thus lowering the score of the
rule.

We have about 50 rule simplifications that are specified as regular
expressions. Table~\ref{tab:rule-simplification} shows the number of rules
generated originally (without simplification) and the number after simplifying,
for different lengths of rules. It is clear that exponential growth is still
present as the rule length increases. However, we benefit by ensuring we are
scoring the normalized rule rather than equivalent variations.

\begin{table}
\centering
\begin{tabular}{|l|l|l|l|}
\hline
Rule length & Original count & Simplified count & Ratio \\
\hline
1 & 313 & 313 & 1.0 \\
2 & 97,000 & 90538 & 0.93 \\
3 & 30,762,000 & 26,726,754 & 0.87 \\
4 & 296,735,000 & 255,805,952 & 0.86 \\
\hline
\end{tabular}
\caption{Impact of rule simplification. Rule length indicates number of
primitives in each complex rule; e.g., `r ] \$1' has length 3. Original count
specifies the number of rules generated with a certain length, without rule
simplification. Simplified count shows number of rules that remain after
rewriting some to a simpler form. Simpler forms will typically be repeats and
will be removed from the count.}
\label{tab:rule-simplification}
\end{table}

We modify the brute-force procedure with this optimization at line~15 by
first simplifying the new complex rule before adding it to the rule history.
This change is shown in Algorithm~\ref{alg:rule-simplification}.

\begin{algorithm}\caption{Rule simplification}
\begin{algorithmic}
    \State $H \gets \{r\}\cup\{\mathrm{simplify}(\mathrm{append}(h, r))|h \in%
\mathrm{ruleHistory}(p)\}$
\end{algorithmic}
\label{alg:rule-simplification}
\end{algorithm}

\subsubsection{No-op rule detection}

We also detect rules that accomplish nothing. For example, the rule `r r'
(reverse, then reverse again) will be boosted repeatedly since it essentially
does not transform a password at all. If the candidate password is already a
target, then the password generated by this rule is also a target (because it
is the same word), so `r r' will be boosted. In effect, the procedure will
yield abundant high-scoring rules that accomplish very little and will not be
effective for cracking hashes. These no-op rules are detected and eliminated as
shown in Algorithm~\ref{alg:eliminate-no-op}

\begin{algorithm}\caption{Eliminate no-op rules}
\begin{algorithmic}
    \State $H \gets H \setminus \{h|h\in H : \textrm{isNoOpRule}(h)\}$
\end{algorithmic}
\label{alg:eliminate-no-op}
\end{algorithm}

\subsubsection{Inventing primitive rules}

In order to facilitate generation of complex rules, we promote a complex rule
to the primitive rule set if the rule produces a target sufficiently often (we
experimentally chose this threshold to be 10 targets). For example, if the
primitive rule `\$3' is added to a rule history containing `\$1 \$2' and the
resulting complex rule `\$1 \$2 \$3' produces a target at least 10 times, then
`\$1 \$2 \$3' is added as a primitive. As a primitive, it will be added as a
single unit to other rules, e.g., it will be added to `\$1 \$2' as in this
example, yielding `\$1 \$2 \$1 \$2 \$3.' In our experimental results section,
we will show how many new primitive rules are invented.

\subsection{Optimizations for time and memory}

Other optimizations ensure our algorithm is time-efficient and uses limited
memory.

\subsubsection{Use of radix trees}

Because a password (potential new candidate) can be reached by many combinations of
primitive rules appled to a candidate, it is important for our procedure to recognize which
of these potential new candidates have already been processed in order to avoid significant
duplicate processing. The naive approach of using a set very quickly becomes untenable with
rapid growth in memory consumption. To mitigate this, our procedure takes advantage of radix trees
to store unprocessed and processed passwords. The substring `password' in 
`password123', `password!1', and `passwordxyz' will only be stored once. This optimization
dramatically slows down memory consumption as our process proceeds, eventually leveling out
at around XXXX GB.

We make use of the same optimization to store our large number of generated rules.

\subsubsection{Capping the candidate set}

The main growth of memory in the brute-force procedure is the result of
generating new password candidates. These candidates are saved to the queue and
processed according to the main loop starting on line~9 of
Algorithm~\ref{alg:brute-force}. In practice, we specify a maximum number of
cycles (i.e., how many times to repeat that loop), and we also choose a batch
of candidates at a time. We typically run for 50,000 cycles and choose 400
candidates at a time. We can compute the number of password candidates that
will ever be examined as the product of these two numbers (20,000,000).
Whenever a password is generated and it was not previously known, it is scored
according to its strength and added to a priority queue. Scores do not change
after candidates are added to the queue, so periodically (say, every 1,000
cycles), we eliminate any members of the queue that are below the 25-millionth
position. This technique allows us to cap the size of the candidate set and
thereby cap the size of memory. While the brute-force procedure suffers from
excessive growth of memory due to the combinatorial nature of password
generation, our more efficient variant limits the resident memory size, thus
allowing the procedure to complete without running out of system memory.

\section{Experimental Methodology}

We chose to use the full rockyou wordlist containing about 14~million plaintext
passwords. This is our target set, and the original set of candidates. In order
to utilize our password strength metric, we require a target list that is
sorted by frequency of occurrence in real-world usage. rockyou is not sorted in
this way, but we can use the Pwned Hashes list, which includes frequencies.
Though Pwned Hashes contains hashes, not plaintext passwords, we can hash each
rockyou password and look up its frequency in the Pwned Hashes list, and order
rockyou by those frequencies.

We ran the algorithm for 50,000 cycles and 400 candidates per cycle, resulting
in 20,000,000 passwords being analyzed.

Our algorithm produces a list of rules. We remove logical duplicates using the `duperule' program~\cite{duprule}, which catches some duplicate rules that our rule simplifier misses. For example, it finds that `r ] \^{}n' (reverse, remove last, add `n' to front) is the same as `\$n r ]' (add `n' to end, reverse, remove last), so the latter rule is removed. With these deduplicated rules, we use Hashcat and the same rockyou wordlist to attempt to crack the most frequent 100~million Pwned Hashes. We record the percent cracked.

We compared performance of our rules against several other lists of rules,
including some that incorporate our own rules:

\begin{itemize}
\item An empty rule list, to see what percentage rockyou itself can crack.
\item Different sizes of our generated rules, ordered by rule score; e.g., top
10,000 rules, top 50,000, etc.
\item The `dive' rules that come with the Hashcat distribution.
\item OneRuleToRuleThemAll with our generated rules appended, then trimmed to the size of the `dive' ruleset (99k rules).
\item Pantagrule's top-performing and largest rules, pantagrule.private.v5.popular.
\item Pantagrule's rules pantagrule.private.v5.popular plus our generated rules, with duplicates removed from the combined set.
\end{itemize}

We also check the number of duplicate rules (according to the `duprule' program) that we share with other rules like OneRuleToRullThemAll, dive, and pantagrule.private.v5.popular.

\section{Results}

our generated rules are intuitively accurate, based on what we think we know
about how people modify their passwords

Table~\ref{tab:top_rules} shows top rules

While it is not the case that selecting only some `top-n' set of rules from our
generated rules results in a clear win against some common similarly-sized rule sets
like dive.rule (99,084 rules) or various Pantagrule rule sets, our results clearly
indicate that we are generating some strong rules that are not included in these
existing rule sets. In rarecoil's analysis of their Pantagrule rule sets they compare
dive.rule to a combination of OneRule and generated Pantagrule rules truncated to
the size of dive to demonstrate the utility of their rules. At the time of its
creation Pantagrule's `one' performed better than known lists equal in size to dive.

Our approach XXXX above compares Pantagrule's highly effective pantagrule.private.v5.one
to a concatenation of OneRule with our generated rules, truncated to the size of dive
and with no literal duplicates between the two sets (for the sake of comparison `functional'
duplicates that could be eliminated by the dedupe tool are not removed as OneRule contains
several of those).  [we are more effective | we are almost as effectve but with different rules;
we demonstrate we're producing useful rules]

Pantagrule also claims minimal (couple thousand rule) overlap between their generated rules and OneRule
and notes that this is interesting because it makes the strategies complementary. The number of our top-XXXX rules
that are a literal overlap with OneRule rules is XXXX so this observation also applies to the rules we produce.

While these comparisons demonstrate the effectiveness of our rules in comparison to the Pantagrule
rule sets, they do not necessarily indicate the effectiveness of our procedure compared to
the procedure used to generate the Pantagrule rule sets. This is because while we used rockyou as
our wordlist, Pantagrule was developed with the use of a public but now inaccessible wordlist.
However, as Pantagrule describes their procedure we can repeat their rule generation using
the same wordlist we used, producing our own version of pantagrule.private.v5.one. Comparing
this to OneRuleToFindThem shows that the rulefile generated with our approach [cracked more passwords,
therefore indicating our procedure is more effective | did not, :(]

\begin{table}
\centering
\begin{tabular}{|l|l|l|}
\hline
Rule & Score & Explanation \\
\hline
\$1 & 508,091 & Add `1' to end \\
T0 & 369,973 & Toggle case of first character \\
\$2 & 355,021 & Add `2' to end \\
t & 313,526 & Toggle case of all characters \\
\$7 & 290,926 & Add `7' to end \\
\$3 & 284,959 & Add `3' to end \\
] & 281,415 & Remove last character \\
\$1 \$2 & 273,308 & Add `12' to end \\
\$5 & 253,183 & Add `5' to end \\
\$4 & 246,386 & Add `4' to end \\
\$s & 239,729 & Add `s' to end \\
\$6 & 232,530 & Add `6' to end \\
\$1 \$2 \$3 & 229,973 & Add `123' to end \\
\hline
\end{tabular}
\caption{Top rules generated by our procedure. Scores represent relative
success at matching target passwords (an approximation of cracking success).}
\label{tab:top_rules}
\end{table}

Figure~\ref{fig:rule-count} shows that the number of complex rules grows per cycle, but gradually levels off. Recall that a complex rule is created when it has never been seen before and is able to transform a candidate password into a target. Over time, fewer rules are generated that are both novel and successful. Also recall that particularly successful rules are promoted to primitives. The frequency of this occurrence also levels off, as shown in the figure.

The `duprule' program~\cite{duprule} eliminated X duplicate rules from our generated set of X rules (X\%). Table~\ref{tab:dups} shows how many deduplicated rules generated by our procedure are also found in various other rule lists. The low `percent dup' values indicate that our generated rules do not have much overlap with existing large rule sets. Thus, our techniques compliment each other, and likely the best cracking performance may be obtained by combining rule sets.

\begin{table}
\centering
\begin{tabular}{|l|l|l|l|}
    \hline
    Rules & Count & Duplicates & Percent dup \\
    \hline
    dive & 99,092& 3,712 & 3.7\% \\
    ORTRTA & 51,998 & 5,318 & 10.2\% \\
    pantagrule popular & 478,736 & 9,764 & 2.0\% \\
    \hline
\end{tabular}
\caption{Counts of rules that are found in both our generated rules and each existing rule set. `ORTRTA' represents the rule set `OneRuleToRuleThemAll'~\cite{ortrta}. `pantagrule popluar' refers to Pantagrule's `pantagrule.private.v5.popular.rule'~\cite{pantagrule}. The `Count' column indicates the count of rules in the rule set, the `Duplicates' column indicates the count of rules in the rule set that match one of our generated rules, and the `Percent dup' column is defined as the `Duplicates' column divided by the `Count' column.}
\label{tab:dups}
\end{table}


\begin{figure}
\includegraphics[width=\linewidth]
{analysis/passwords-analysis/stats-rules_composites_size.pdf}
\caption{Growth of complex and primitve rules over time (cycles). As targets
are hit, more complex rules are added. }
\label{fig:rule-count}
\end{figure}

\begin{figure}
    \includegraphics[width=\linewidth]
    {../cracked_attempted_plot_08318a9a-a503-11ed-9d73-005056c00001.png}
    \caption{NOT FINAL}
    \label{fig:cracked-attempted}
\end{figure}

memory usage was capped, due to our optimizations

\section{Future Work}

explore use of word dictionaries

boost rule score only if rule produced a stronger password from a weaker one?

\section*{Acknowledgments}

We wish to thank Jonathan Lamoureux for his contributions to this project.

\section*{Availability}

Our code and results are available on GitHub at
\texttt{github.com/joshuaeckroth/passwords}. We used various datasets
to generate our results:

\begin{itemize}
\item RockYou plaintext passwords:
\texttt{github.com/ zacheller/rockyou}
\item Pwned Passwords version 8, ordered by prevalence:
\texttt{haveibeenpwned.com/Passwords}
\item Pantagrule rules: \texttt{github.com/rarecoil/pantagrule}
\end{itemize}

Hashcat was used to measure the performance of rules:
\texttt{github.com/hashcat/hashcat}.
We also used
`duprule,' a duplicate rule detector:
\texttt{github.com/mhasbini/duprule}.


%-------------------------------------------------------------------------------
\bibliographystyle{plain}
\bibliography{\jobname}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%  LocalWords:  endnotes includegraphics fread ptr nobj noindent
%%  LocalWords:  pdflatex acks
